---
title: A Tidyverse workflow
author: Jamaal Green
date: '2018-07-30'
slug: a-tidyverse-workflow
categories: []
tags: []
description: 'Dates, times and summary stats, oh my!'
---

We've discussed a lot of the basics and just started an introduction to the tidyverse. There is no better way to get acquainted with a fuller range of these tools than by digging into a real dataset and showcasing the power.

Thus far, we've worked with some smaller example datasets, but now we will be working with a dataset of > 1 million rows of 911 calls. Our mission is to import this data, clean it up a bit, calculate some summary statistics of interest and do some basic data visualization. At the end, we will export our newly created cleaned dataset both to a csv and RDS file so we can get an idea of the memory differences between the formats.

##The Data

This is emergency call data from the [Budget Office of the City of Portland](https://www.portlandoregon.gov/cbo/). It has only five columsn but nearly 1.4 million rows as it has nearly 3 years of call data. Like many datasets, this data comes in an especially hefty 45MB excel file and is spread over four sheets (download [here]("Master Data_thru_June_Nick Kobel.xlsx")).

Fortunately, the data is pretty well formatted already so we do not have to do extensive cleaning, but we still need to make sure our dates are in the proper format and our variables are of the proper types. Once that is done, we would like to calculate a series of summary statistics (by increasing difficulty/complexity): the total number of emergency calls; the number of emergency calls by year; the number of calls by month, by year; and the the proportion of calls by cell phone or landline by year.

We will be making primarily use of the **lubridate** and **dplyr** packages for cleaning and summarizing and the **rio** package for importing our sheets (I'll also be using the **here** package for easier calling of files, but it is not strictly necessary).

###Import

First, we gotta these sheets into R. We've already done a quick visual inspection in excel so we know we don't have any obviously apparent issues regarding columns, but note that the analysts added `NULL` in cells instead of leaving them blank. This will matter when importing because R will read `NULL` as a **string** and covnert every other value in that column to string. While it's usually easy to convert column types once our data is imported if you don't notice this it can easily mess up your analysis down the line. So, beware.

```{r echo=TRUE, message=FALSE, warning=FALSE}


if(!require(pacman)){install.packages("pacman"); library(pacman)}
p_load(here, rio, lubridate, dplyr)

boecxls1 <- import(here::here("static/Master Data_thru_June_Nick Kobel.xlsx"), which = 1, na = c("", "NULL")) 
boecxls2 <- import(here::here("static/Master Data_thru_June_Nick Kobel.xlsx"), which = 2, na = c("", "NULL")) 
boecxls3 <- import(here::here("static/Master Data_thru_June_Nick Kobel.xlsx"), which = 3, na = c("", "NULL")) 
boecxls4 <- import(here::here("static/Master Data_thru_June_Nick Kobel.xlsx"), which = 4, na = c("", "NULL")) 

boecxls <- bind_rows(boecxls1, boecxls2, boecxls3, boecxls4)

glimpse(boecxls)

```

Don't worry too much over that beginning set of commands. That's a little convenience snippet I have to call **pacman** a package management package. It's p_load() function checks to see if a package is installed and to load it. 

So, we know our workbook has four sheets, the first with nearly 2 years of calls and the next three broken down monthly. In my import calls I'm pulling in each individual sheet (this should look familiar), but I've added a new call in the functon-- the "na" argument. This is an argument that allows us to declare what values present in the sheet stand for NULLs. In this case, I said that blank cells " " and the string "NULL" all signify `NA`. This will help prevent any weird conversion issues from that NULl issue I mentioned above. And because the column positions and names are consistent across the sheets I use **dplyr's** `bind_rows()` function to stick everything together making one tall table of calls. Finally, I use the `glimpse` function from **dplyr** again to get a basic idea of the columns in my data frame look like. 